# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNwRlaQSxnxbRw7ZzAm9cscDpHF1C8B8

##### Copyright 2024 Google LLC.

### Install the Python SDK

The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:
"""
import pip
pip.main(['install', '-q', '-U', 'google-generativeai'])

#!pip install -q -U google-generativeai

"""### Import packages

Import the necessary packages.
"""

import pathlib
import textwrap

import google.generativeai as genai

from IPython.display import display
from IPython.display import Markdown


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.
GOOGLE_API_KEY='AIzaSyD9QmJ1VOSKP1JGWxf0dF0JWiOzoeAo4L0'

genai.configure(api_key=GOOGLE_API_KEY)

"""## List models

Now you're ready to call the Gemini API. Use `list_models` to see the available Gemini models:

* `gemini-1.5-pro`: optimized for high intelligence tasks, the most powerful Gemini model
* `gemini-1.5-flash`: optimized for multi-modal use-cases where speed and cost are important
"""

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

"""## Generate text from text inputs

For text-only prompts, use the `gemini-pro` model:
"""

model = genai.GenerativeModel('gemini-1.5-pro')

#pip install streamlit google-generativeai openai tenacity
pip.main(['install', '-q', 'streamlit', 'google-generativeai', 'openai', 'tenacity'])
#!pip install pyngrok

import streamlit as st

from pyngrok import ngrok

# Function to start ngrok tunnel (called before app launch)
def start_ngrok_tunnel():
  # Access the port directly, assuming it's set in your Streamlit secrets management.
  port = st.secrets["streamlit_port"]
  tunnel = ngrok.connect(port)
  return tunnel.public_url

# Start ngrok tunnel in the background (if desired)
tunnel_url = None
if st.checkbox("Enable Public Access (requires ngrok)", value=False):
  try:
    tunnel_url = start_ngrok_tunnel()
    st.info(f"Public URL: {tunnel_url}")
  except Exception as e:
    st.error(f"Failed to start ngrok tunnel: {e}")


st.title("Tourist Planner")

# Get user input with Streamlit elements
destination = st.text_input("Enter the place you want to visit:")
num_people = st.number_input("Enter the number of people traveling:", min_value=1)
num_days = st.number_input("Enter the number of days of your stay:", min_value=1)
budget = st.selectbox("Select your budget:", ["Low", "Medium", "High"])

# Get user's interests with checkboxes
interests = []
st.subheader("Select your interests:")
if st.checkbox("History"):
  interests.append("history")
if st.checkbox("Food"):
  interests.append("food")
if st.checkbox("Adventure"):
  interests.append("adventure")
if st.checkbox("Art"):
  interests.append("art")
if st.checkbox("Nightlife"):
  interests.append("nightlife")

# Build the prompt incorporating all user input
interest_string = ", ".join(interests) if interests else "no specific interests"
prompt = f"Suggest a {num_days}-day itinerary for {num_people} people visiting {destination}, with a budget of {budget}. Include interesting tourist spots that cater to {interest_string}."

# Generate response and display using Markdown
if st.button("Generate Itinerary"):
  response = model.generate_content(prompt)
  print(response.text)


# Stop ngrok tunnel when app closes (if applicable)
if tunnel_url:
  ngrok.kill()

st.stop()

